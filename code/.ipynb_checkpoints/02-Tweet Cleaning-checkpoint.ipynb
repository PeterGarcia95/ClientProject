{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_outages='../datasets/raw_tweets.csv'\n",
    "file_name_random='../datasets/irrelevent_tweets.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(file_name_outages).drop(columns='Unnamed: 0')\n",
    "df1['label']='outage'\n",
    "df2=pd.read_csv(file_name_random).drop(columns='Unnamed: 0')\n",
    "df2['label']='irrelevant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df1,df2],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dates']=pd.to_datetime(df.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dates']=pd.to_datetime(df.dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username            object\n",
       "text                object\n",
       "date                object\n",
       "label               object\n",
       "dates       datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>JulieWilcoxWX</td>\n",
       "      <td>An emergency manager once explained it this wa...</td>\n",
       "      <td>outage</td>\n",
       "      <td>2019-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>hays_wood</td>\n",
       "      <td>@Xfinity get your shit together! First college...</td>\n",
       "      <td>outage</td>\n",
       "      <td>2019-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>EdValleeWx</td>\n",
       "      <td>Have had many ask me what to do for prep. Here...</td>\n",
       "      <td>outage</td>\n",
       "      <td>2019-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>teamworkf1</td>\n",
       "      <td>@RogersHelps - GM, is any outage in the Duffer...</td>\n",
       "      <td>outage</td>\n",
       "      <td>2019-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>shellz_woo</td>\n",
       "      <td>Solar lights that people use outside to light ...</td>\n",
       "      <td>outage</td>\n",
       "      <td>2019-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        username                                               text   label  \\\n",
       "0  JulieWilcoxWX  An emergency manager once explained it this wa...  outage   \n",
       "1      hays_wood  @Xfinity get your shit together! First college...  outage   \n",
       "2     EdValleeWx  Have had many ask me what to do for prep. Here...  outage   \n",
       "3     teamworkf1  @RogersHelps - GM, is any outage in the Duffer...  outage   \n",
       "4     shellz_woo  Solar lights that people use outside to light ...  outage   \n",
       "\n",
       "       dates  \n",
       "0 2019-08-31  \n",
       "1 2019-08-31  \n",
       "2 2019-08-31  \n",
       "3 2019-08-31  \n",
       "4 2019-08-31  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how='any',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words and HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_cleaning(raw_tweet):\n",
    "    \n",
    "    # 1. Remove HTML\n",
    "    tweet_text = BeautifulSoup(raw_tweet).get_text()\n",
    "    \n",
    "    # 2. Remove punctuation, keep numbers with 1-4 digits\n",
    "    letters_numbers_only = re.sub(\"[^a-zA-Z]\", \" \", tweet_text)\n",
    "    latin = re.sub(\"r'[\\p{Latin}]'\", \" \", letters_numbers_only)\n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words.\n",
    "    words = latin.lower().split()\n",
    "    \n",
    "    # 4. Search\n",
    "    stops = set(stopwords)\n",
    "    \n",
    "    # 5. Stop Words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # 6. Join \n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 61263 tweets.\n"
     ]
    }
   ],
   "source": [
    "# Tweet Count\n",
    "total_tweets = df.shape[0]\n",
    "print(f'There are {total_tweets} tweets.')\n",
    "\n",
    "# List holders\n",
    "clean_train_tweets = []\n",
    "clean_test_tweets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set movie reviews...\n",
      "Tweet 1000 of 61263.\n",
      "Tweet 2000 of 61263.\n",
      "Tweet 3000 of 61263.\n",
      "Tweet 4000 of 61263.\n",
      "Tweet 5000 of 61263.\n",
      "Tweet 6000 of 61263.\n",
      "Tweet 7000 of 61263.\n",
      "Tweet 8000 of 61263.\n",
      "Tweet 9000 of 61263.\n",
      "Tweet 10000 of 61263.\n",
      "Tweet 11000 of 61263.\n",
      "Tweet 12000 of 61263.\n",
      "Tweet 13000 of 61263.\n",
      "Tweet 14000 of 61263.\n",
      "Tweet 15000 of 61263.\n",
      "Tweet 16000 of 61263.\n",
      "Tweet 17000 of 61263.\n",
      "Tweet 18000 of 61263.\n",
      "Tweet 19000 of 61263.\n",
      "Tweet 20000 of 61263.\n",
      "Tweet 21000 of 61263.\n",
      "Tweet 22000 of 61263.\n",
      "Tweet 23000 of 61263.\n",
      "Tweet 24000 of 61263.\n",
      "Tweet 25000 of 61263.\n",
      "Tweet 26000 of 61263.\n",
      "Tweet 27000 of 61263.\n",
      "Tweet 28000 of 61263.\n",
      "Tweet 29000 of 61263.\n",
      "Tweet 30000 of 61263.\n",
      "Tweet 31000 of 61263.\n",
      "Tweet 32000 of 61263.\n",
      "Tweet 33000 of 61263.\n",
      "Tweet 34000 of 61263.\n",
      "Tweet 35000 of 61263.\n",
      "Tweet 36000 of 61263.\n",
      "Tweet 37000 of 61263.\n",
      "Tweet 38000 of 61263.\n",
      "Tweet 39000 of 61263.\n",
      "Tweet 40000 of 61263.\n",
      "Tweet 41000 of 61263.\n",
      "Tweet 42000 of 61263.\n",
      "Tweet 43000 of 61263.\n",
      "Tweet 44000 of 61263.\n",
      "Tweet 45000 of 61263.\n",
      "Tweet 46000 of 61263.\n",
      "Tweet 47000 of 61263.\n",
      "Tweet 48000 of 61263.\n",
      "Tweet 49000 of 61263.\n",
      "Tweet 50000 of 61263.\n",
      "Tweet 51000 of 61263.\n",
      "Tweet 52000 of 61263.\n",
      "Tweet 53000 of 61263.\n",
      "Tweet 54000 of 61263.\n",
      "Tweet 55000 of 61263.\n",
      "Tweet 56000 of 61263.\n",
      "Tweet 57000 of 61263.\n",
      "Tweet 58000 of 61263.\n",
      "Tweet 59000 of 61263.\n",
      "Tweet 60000 of 61263.\n",
      "Tweet 61000 of 61263.\n",
      "Cleaning and parsing the testing set ...\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Running the functions\n",
    "print(\"Cleaning and parsing the training set movie reviews...\")\n",
    "\n",
    "j = 0\n",
    "for train_tweet in df['text']:\n",
    "    # Join clean reviews\n",
    "    clean_train_tweets.append(tweet_cleaning(train_tweet))\n",
    "    \n",
    "    # Message to keep track\n",
    "    if (j + 1) % 1000 == 0:\n",
    "        print(f'Tweet {j + 1} of {total_tweets}.')\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "print(\"Cleaning and parsing the testing set ...\")\n",
    "warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text']=clean_train_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>dates</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50013</td>\n",
       "      <td>kaedenoha_89</td>\n",
       "      <td>待ってどれwwww</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td>wwww</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50014</td>\n",
       "      <td>hhacffcvhim</td>\n",
       "      <td>للآن يتصالحون واالشعوب مركونين بالفراغ المميت ...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50015</td>\n",
       "      <td>hito_yr</td>\n",
       "      <td>Jimin in Paris, official video \\n#JIMINpic.twi...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td>jimin paris official video jiminpic twitter co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50016</td>\n",
       "      <td>joelbirch</td>\n",
       "      <td>Your name is MŸKKECÅRLT now. You will require ...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td>name kkec rlt require assembly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50017</td>\n",
       "      <td>takapzdra91</td>\n",
       "      <td>まひちゃん熱中症ならなくてよかった(๑•ᴗ•๑)</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           username                                               text  \\\n",
       "50013  kaedenoha_89                                          待ってどれwwww   \n",
       "50014   hhacffcvhim  للآن يتصالحون واالشعوب مركونين بالفراغ المميت ...   \n",
       "50015       hito_yr  Jimin in Paris, official video \\n#JIMINpic.twi...   \n",
       "50016     joelbirch  Your name is MŸKKECÅRLT now. You will require ...   \n",
       "50017   takapzdra91                           まひちゃん熱中症ならなくてよかった(๑•ᴗ•๑)   \n",
       "\n",
       "            label      dates  \\\n",
       "50013  irrelevant 2019-08-31   \n",
       "50014  irrelevant 2019-08-31   \n",
       "50015  irrelevant 2019-08-31   \n",
       "50016  irrelevant 2019-08-31   \n",
       "50017  irrelevant 2019-08-31   \n",
       "\n",
       "                                              clean_text  \n",
       "50013                                               wwww  \n",
       "50014                                                     \n",
       "50015  jimin paris official video jiminpic twitter co...  \n",
       "50016                     name kkec rlt require assembly  \n",
       "50017                                                     "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how='any',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PETER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=pd.read_csv('./Outages_since_2014.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and each value in the nested list is a date that the blackout occured\n",
    "blackouts = [pd.date_range(start = targets.loc[i ,'Date Event Began'], end = targets.loc[i, 'Date of Restoration'])for i in targets.index]\n",
    "\n",
    "# Flatten the 2d list to 1d\n",
    "# https://www.geeksforgeeks.org/python-ways-to-flatten-a-2d-list/\n",
    "blackouts = [j for sub in blackouts for j in sub]\n",
    "\n",
    "# Number of tweets that occured during an actual blackout\n",
    "df['dates'].isin(blackouts).sum()\n",
    "\n",
    "df['target'] = df['dates'].isin(blackouts)\n",
    "df['target'] *= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance Fix - Sampling With Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakenews = df[df['target'] == 0]\n",
    "fakenews_subset = fakenews.sample(n = 1000, random_state=42)\n",
    "fakenews_subset\n",
    "\n",
    "final_df = pd.concat([df[df['target'] == 1], fakenews_subset], axis = 0)\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitudes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating Random Boston Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coor_lat=range(42229077, 42397652)\n",
    "latitudes=np.random.choice(coor_lat, size=final_df.shape[0])\n",
    "latitudes=latitudes/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coor_long=range(-71893220, -70987133)\n",
    "longitude=np.random.choice(coor_long, size=final_df.shape[0])\n",
    "longitude=longitude/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['lat']=latitudes\n",
    "final_df['long']=longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('./clean_tweets.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
